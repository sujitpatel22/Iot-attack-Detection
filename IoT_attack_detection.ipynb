{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsGKmpo6vAKVqYHIzlE7ff",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujitpatel22/Iot-attack-Detection/blob/main/IoT_attack_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading Dependencies**"
      ],
      "metadata": {
        "id": "wEBivTcH5jh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense, LSTM\n",
        "from tensorflow.keras.layers import Dropout, Lambda, Reshape, Concatenate, Flatten\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow import expand_dims\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import csv\n",
        "import ipaddress\n",
        "import sys"
      ],
      "metadata": {
        "id": "55RoF26c5sdI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading Data**"
      ],
      "metadata": {
        "id": "EdG3tt153r1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the Dataset using Pandas\n",
        "def load_data(filename):\n",
        "  dataFrame = pd.read_csv(filename)\n",
        "  return dataFrame"
      ],
      "metadata": {
        "id": "qWt6uMQX34Qi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pre-processing of Dataset**"
      ],
      "metadata": {
        "id": "odhYMfmr6-yV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preProcess(data, num_classes):\n",
        "  data[\"class\"] = to_categorical(data[\"class\"].astype(int), num_classes)\n",
        "  data = convert_ip_to_integer(data, 'Sender_IP', 'Target_IP')\n",
        "\n",
        "  # remove any row having [\"nan\",\"infinity\",\"null\",\" \",\"NaN\"] in it.\n",
        "  for index, row in data.iterrows():\n",
        "    for col_name in data.columns:\n",
        "      if row[col_name] in [\"nan\",\"infinity\",\"null\",\" \",\"NaN\"]:\n",
        "        data = data.drop(index)\n",
        "        break;\n",
        "\n",
        "  # Convert the non-numeric columns in to int64/numeric values\n",
        "  data = convert_id_to_int(data)\n",
        "  data['ID'] = data['ID'].astype(int)\n",
        "  # Apply MinMaxScaler to numeric columns\n",
        "  numeric_columns = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "  scaler = MinMaxScaler()\n",
        "  data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
        "  data.to_csv('Datasets_refined.csv', header = False, index=False)\n",
        "\n",
        "# Convert the IP_addresses columns into numeric\n",
        "def convert_ip_to_integer(data, Sender_IP, Target_IP):\n",
        "  data[Sender_IP] = data[Sender_IP].apply(lambda x: int(ipaddress.IPv4Address(x)))\n",
        "  data[Target_IP] = data[Target_IP].apply(lambda x: int(ipaddress.IPv4Address(x)))\n",
        "  return data\n",
        "\n",
        "# Convert the ID column into numeric column of integer\n",
        "def convert_id_to_int(data):\n",
        "  for index, row in data.iterrows():\n",
        "    splited_id = str(row[\"ID\"]).split(\"-\")\n",
        "    converted_id = []\n",
        "    for token in splited_id:\n",
        "      converted_token = int(ipaddress.IPv4Address(token)) if not token.isdigit() else int(token)\n",
        "      converted_id.append(converted_token)\n",
        "    merged_id, pow = 0, 0\n",
        "    for c_id in converted_id:\n",
        "      merged_id += c_id*(256**pow)\n",
        "      pow += 1\n",
        "    data.at[index, \"ID\"] = int(merged_id)\n",
        "  return data\n",
        "\n"
      ],
      "metadata": {
        "id": "7V6_8GhD6XTa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spliting the  dataset into X & Y values"
      ],
      "metadata": {
        "id": "v_Z9hQ6eiquV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spliting the Preprocessed Dataset into X input data and Y truth labels\n",
        "# returning X, Y as numpy arrays.\n",
        "def load_processed_data(processed_csv_name):\n",
        "  data = pd.read_csv(processed_csv_name, header = None)\n",
        "  X = []\n",
        "  Y = []\n",
        "  X = data.iloc[:, :-1]  # Select all columns except the last one\n",
        "  Y = data.iloc[:, -1] # Select the last column as target label column\n",
        "  return np.array(X),np.array(Y)"
      ],
      "metadata": {
        "id": "0UBcu5NkYeYc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building the Model**"
      ],
      "metadata": {
        "id": "zPGFavdlr6ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the DNN model\n",
        "def create_dnn_model(num_features):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(400, activation = \"relu\", kernel_initializer=\"he_normal\", name = \"dense1\"))\n",
        "  model.add(Dense(100, activation = \"relu\", kernel_initializer=\"he_normal\", name = \"dense2\"))\n",
        "  model.add(Dense(50, activation = \"relu\", kernel_initializer=\"he_normal\", name = \"dense3\"))\n",
        "  model.add(Flatten())\n",
        "  return model\n",
        "\n",
        "# Creating the LSTM model\n",
        "def create_lstm_model(num_features):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(400, return_sequences=True, name=\"lstm1\"))\n",
        "  model.add(LSTM(100, return_sequences=True, name=\"lstm2\"))\n",
        "  model.add(LSTM(50, return_sequences=False, name=\"lstm3\"))\n",
        "  return model\n",
        "\n",
        "# Main building function to merge the DNN & LSTM models to build the main model\n",
        "def build_main_model(num_features, num_classes):\n",
        "  # Input layer to feed data into the DNN & the LSTM model\n",
        "  input_layer = Input(shape = (1, num_features), dtype = 'float32', name=\"input_layer\")\n",
        "\n",
        "  # Create DNN & LSTM models\n",
        "  dnn_model = create_dnn_model(num_features)\n",
        "  lstm_model = create_lstm_model(num_features)\n",
        "\n",
        "  # define output layers for both the models\n",
        "  dnn_output = dnn_model(input_layer)\n",
        "  # print(\"dnn_ouput:\", dnn_output.shape)\n",
        "  lstm_output = lstm_model(input_layer)\n",
        "  # print(\"lstm_ouput:\", lstm_output.shape)\n",
        "\n",
        "  # Merge both the model's outputs to one single output with same tensor shape as the individual tensor shape\n",
        "  merged_layer = Concatenate()([dnn_output, lstm_output])\n",
        "  dense = Dense(40, activation = \"relu\", kernel_initializer=\"he_normal\", name = \"dense4\")(merged_layer)\n",
        "  dense = Dense(15, activation = \"relu\", kernel_initializer=\"he_normal\", name = \"dense5\")(dense)\n",
        "  # Using Singmoid activation fucntion in the output layer for Binary_classification as in our specific case\n",
        "  output_layer = (Dense(1, activation = \"sigmoid\", name=\"output_layer\"))(dense)\n",
        "\n",
        "  model = Model(inputs = input_layer, outputs = output_layer)\n",
        "  return model"
      ],
      "metadata": {
        "id": "yaEjcrKhCiLd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Compile Model**"
      ],
      "metadata": {
        "id": "ko6pTq-xj828"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model using 'adam' optimizer and 'binary_crossentropy' loss\n",
        "def compile_model(model):\n",
        "  model.compile(\n",
        "      optimizer = \"adam\",\n",
        "      loss = 'binary_crossentropy',\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "  return model"
      ],
      "metadata": {
        "id": "0EU9PyjJkCaD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluating Model**"
      ],
      "metadata": {
        "id": "6hu-DcdjSksh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, Y_train, Y_test):\n",
        "  accuracy = model.evaluate(Y_train, Y_test)\n",
        "  accuracy = \"{:.3f}\".format(accuracy[1] * 100)\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "pkMeQmIdSqaw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The main driver function**"
      ],
      "metadata": {
        "id": "Y4skcg9HBfbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NUM_FEATURES = no of features in the input_data vector\n",
        "# NUM_CLASSES = no of classes for classification\n",
        "NUM_FEATURES, NUM_CLASSES = 18, 2\n",
        "\n",
        "def main():\n",
        "  data = load_data(\"Datasets.csv\")\n",
        "  preProcess(data, NUM_CLASSES)\n",
        "  X, Y = load_processed_data(\"Datasets_refined.csv\")\n",
        "\n",
        "  # Convert X data and Y labels of dtype =  np.float32\n",
        "  X = X.astype(np.float32)\n",
        "  Y = Y.astype(np.float32)\n",
        "  X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
        "  # print(X.shape)\n",
        "  # Spliting the X, Y data into 90% training data & 10% Testing data\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.10)\n",
        "\n",
        "  model = build_main_model(NUM_FEATURES, NUM_CLASSES)\n",
        "  model = compile_model(model)\n",
        "  # Model is trained for 20 Epochs for even better prediction.\n",
        "  model.fit(X_train, Y_train, epochs = 20, verbose = 2)\n",
        "\n",
        "  # saving the model\n",
        "  model.save(\"model.h5\")\n",
        "  # Evaluating the model on the testing dataset\n",
        "\n",
        "  accuracy = evaluate(model, X_test, Y_test)\n",
        "  print(\"Overall Model accuracy: \", accuracy, \"%\")\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "TRXzg_36BjsI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80bee632-417c-4084-d319-b4709f2b7a51"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_layer_shape: (None, 1, 18)\n",
            "Epoch 1/20\n",
            "154/154 - 6s - loss: 0.4813 - accuracy: 0.7638 - 6s/epoch - 41ms/step\n",
            "Epoch 2/20\n",
            "154/154 - 2s - loss: 0.3843 - accuracy: 0.7937 - 2s/epoch - 15ms/step\n",
            "Epoch 3/20\n",
            "154/154 - 2s - loss: 0.3684 - accuracy: 0.7973 - 2s/epoch - 12ms/step\n",
            "Epoch 4/20\n",
            "154/154 - 2s - loss: 0.3332 - accuracy: 0.8032 - 2s/epoch - 12ms/step\n",
            "Epoch 5/20\n",
            "154/154 - 2s - loss: 0.3199 - accuracy: 0.8193 - 2s/epoch - 10ms/step\n",
            "Epoch 6/20\n",
            "154/154 - 2s - loss: 0.3096 - accuracy: 0.8286 - 2s/epoch - 11ms/step\n",
            "Epoch 7/20\n",
            "154/154 - 2s - loss: 0.2987 - accuracy: 0.8280 - 2s/epoch - 11ms/step\n",
            "Epoch 8/20\n",
            "154/154 - 2s - loss: 0.2824 - accuracy: 0.8412 - 2s/epoch - 11ms/step\n",
            "Epoch 9/20\n",
            "154/154 - 2s - loss: 0.2766 - accuracy: 0.8406 - 2s/epoch - 14ms/step\n",
            "Epoch 10/20\n",
            "154/154 - 2s - loss: 0.2734 - accuracy: 0.8475 - 2s/epoch - 12ms/step\n",
            "Epoch 11/20\n",
            "154/154 - 2s - loss: 0.2640 - accuracy: 0.8505 - 2s/epoch - 11ms/step\n",
            "Epoch 12/20\n",
            "154/154 - 2s - loss: 0.2621 - accuracy: 0.8515 - 2s/epoch - 11ms/step\n",
            "Epoch 13/20\n",
            "154/154 - 2s - loss: 0.2618 - accuracy: 0.8422 - 2s/epoch - 10ms/step\n",
            "Epoch 14/20\n",
            "154/154 - 2s - loss: 0.2564 - accuracy: 0.8495 - 2s/epoch - 10ms/step\n",
            "Epoch 15/20\n",
            "154/154 - 2s - loss: 0.2518 - accuracy: 0.8536 - 2s/epoch - 10ms/step\n",
            "Epoch 16/20\n",
            "154/154 - 2s - loss: 0.2474 - accuracy: 0.8552 - 2s/epoch - 13ms/step\n",
            "Epoch 17/20\n",
            "154/154 - 2s - loss: 0.2481 - accuracy: 0.8546 - 2s/epoch - 12ms/step\n",
            "Epoch 18/20\n",
            "154/154 - 2s - loss: 0.2459 - accuracy: 0.8556 - 2s/epoch - 11ms/step\n",
            "Epoch 19/20\n",
            "154/154 - 2s - loss: 0.2510 - accuracy: 0.8519 - 2s/epoch - 11ms/step\n",
            "Epoch 20/20\n",
            "154/154 - 2s - loss: 0.2460 - accuracy: 0.8584 - 2s/epoch - 10ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18/18 [==============================] - 1s 4ms/step - loss: 0.2164 - accuracy: 0.8814\n",
            "Overall Model accuracy:  88.139 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5jw5oa8Tw56c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}